{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c61d41b",
   "metadata": {},
   "source": [
    "# Crop Landcover Exploration\n",
    "\n",
    "This notebook contains some exploratory data analysis for this problem. Our goal is to build and understand a model for predicting crop types for fields in South Africa, based on satellite imagery from the [Sentinel 2 Level 2-A](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a) product.\n",
    "\n",
    "Our labels come from the Radiant Earth [South Africa Crop Type Competition](https://registry.mlhub.earth/10.34911/rdnt.j0co8q/). They're a collection of scenes, with integers indicating the crop type at each pixel in the scene.\n",
    "\n",
    "Our training data comes from Microsoft's Planetary Computer. The [Sentinel 2 Level 2-A](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a) page describes what all is avaiable.\n",
    "\n",
    "## Data Access\n",
    "\n",
    "We'll use [STAC](https://stacspec.org/) (SpatioTemporal Asset Catalog) to easily find and load the data we care about. We'll interact with two STAC catalogs\n",
    "\n",
    "1. A catalog for the labels, hosted in a Blob Storage container\n",
    "2. The Planetary Computer's STAC API, which catalogs all of Sentinel 2 Level 2-A (among [many other collections](https://planetarycomputer.microsoft.com/catalog))\n",
    "\n",
    "The overall workflow will be\n",
    "\n",
    "1. Load a \"chip\" with the label data (a 256x256 array of integer codes indicate the crop type)\n",
    "2. Search for and load a scene with Sentinel 2 imagery covering the `labels` chip\n",
    "3. Transform and crop the (very large) Sentinel 2 scene to match the 256x256 label scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac\n",
    "import pystac_client\n",
    "import requests\n",
    "import shapely.geometry\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \"Creating an ndarray from ragged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f87bc8",
   "metadata": {},
   "source": [
    "### Load Labels\n",
    "\n",
    "We have a STAC catalog of labels for the training data, which is based off the collection used in the Radiant Earth competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2951813",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_catalog = pystac.read_file(\n",
    "    \"https://esip2021.blob.core.windows.net/esip2021/\" \"train/collection.json\"\n",
    ")\n",
    "training_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7c814",
   "metadata": {},
   "source": [
    "That catalog has links to a bunch of Items, where each item represents a scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_links = training_catalog.get_item_links()\n",
    "len(label_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3481f4",
   "metadata": {},
   "source": [
    "We can get the label `Item` by following the link. Item 18 happens to look interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_item = label_links[18].resolve_stac_object().target\n",
    "label_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06df9f6",
   "metadata": {},
   "source": [
    "### Exercise: Explore the pystac.Item\n",
    "\n",
    "Let's explore this `Item` a bit. We could [read the docs](https://pystac.readthedocs.io/en/latest/), but where's the fun in that?\n",
    "\n",
    "* What's the item's bounding box? (See https://pystac.readthedocs.io/en/latest/api.html#pystac.Item.bbox)\n",
    "* When was the item captured? (See https://pystac.readthedocs.io/en/latest/api.html#pystac.Item.datetime)\n",
    "* What assets does the item have? (See https://pystac.readthedocs.io/en/latest/api.html#pystac.Item.assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a206a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the item's bounding box?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/pystac_item_bbox.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the item's date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf056eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the item's date?\n",
    "%load solutions/pystac_item_date.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what assets does the item have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17768582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what assets does the item have?\n",
    "%load solutions/pystac_item_assets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d486676",
   "metadata": {},
   "source": [
    "We're interested in the `labels` asset, which has the Cloud Optimized GeoTiff of integers indicating the crop type at each pixel. Let's load the URL pointed to by that asset into an `xarray.DataArray` using `rioxarray.open_rasterio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b969a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "\n",
    "label_item = training_catalog.links[19].resolve_stac_object().target\n",
    "labels = rioxarray.open_rasterio(\n",
    "    label_item.assets[\"labels\"].get_absolute_href()\n",
    ").squeeze()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816e36c",
   "metadata": {},
   "source": [
    "Let's plot the `labels` to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13528482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac.extensions.label import LabelExtension\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ext = LabelExtension.ext(label_item)\n",
    "r = requests.get(label_item.assets[\"raster_values\"].get_absolute_href())\n",
    "classes = list(r.json().values())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "p = labels.plot.imshow(\n",
    "    ax=ax, cmap=\"tab10\", add_colorbar=False, vmin=0, vmax=len(classes)\n",
    ")\n",
    "cbar = plt.colorbar(p)\n",
    "cbar.set_ticks(np.arange(0.5, len(classes) + 0.5))\n",
    "cbar.set_ticklabels(classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc8414",
   "metadata": {},
   "source": [
    "### Load Sentinel-2 Level 2-A Scenes\n",
    "\n",
    "As we discovered, that item is from a specific date / time. We'll search for Sentinel-2 scenes in the Planetary Computer's catalog overlapping with our training labels around that datetime. We'll use `pystac-client` to make this easier. First, we define a client to work with the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_client = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd12c0f",
   "metadata": {},
   "source": [
    "### Exercise: Search for matching scenes\n",
    "\n",
    "Use `stac_client.search` to search Planetary Computer STAC catalog for matching scenes. Find scenes taht\n",
    "\n",
    "1. Are from the Sentinel-2 Level 2-A collection (hint: specify `collections` as a list)\n",
    "2. Cover the bounding box of `label_item` (hint: specify `bbox`)\n",
    "3. Are from the month before and after the datetime of `label_item` (hint: specify `datetime` as a range like `<start>/<end>`)\n",
    "\n",
    "Then get the matching items with `search.get_all_items()`.\n",
    "\n",
    "How many items match your search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75420ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = stac_client.search(\n",
    "    collections=...,\n",
    "    bbox=...,\n",
    "    datetime=...,\n",
    "    limit=500,  # fetch in batches of 500\n",
    ")\n",
    "items = search.get_all_items()\n",
    "# how many items match your search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9cf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/search.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926fa51",
   "metadata": {},
   "source": [
    "Let's find a scene with few clouds. That information is available under the `EOExtension`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c954d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac.extensions.eo import EOExtension\n",
    "\n",
    "sentinel_item = sorted(items, key=lambda item: EOExtension.ext(item).cloud_cover)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324b785",
   "metadata": {},
   "source": [
    "And now visualize the Sentinel scene, overlaying the bounding box of the label scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, TileLayer, GeoJSON\n",
    "\n",
    "center = shapely.geometry.shape(label_item.geometry).centroid.bounds[:2][::-1]\n",
    "\n",
    "m = Map(center=center, zoom=12)\n",
    "layer = TileLayer(\n",
    "    url=requests.get(sentinel_item.assets[\"tilejson\"].href).json()[\"tiles\"][0],\n",
    ")\n",
    "m.add_layer(layer)\n",
    "m.add_layer(GeoJSON(data=label_item.geometry))\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d17c4",
   "metadata": {},
   "source": [
    "We have a bit of work to align the training labels and the image. First, the coordinate reference system (`epsg` code) doesn't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac.extensions.projection import ProjectionExtension\n",
    "\n",
    "print(\"  Labels CRS:\", labels.rio.crs.to_epsg())\n",
    "print(\"Sentinel CRS:\", ProjectionExtension.ext(sentinel_item).epsg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788dc15d",
   "metadata": {},
   "source": [
    "We'll try to do everything in the coordinates of the labels. As we load the training data into a DataArray, we'll reproject it to the labels' CRS. We'll also trim it down to just the bit we need, covering the labels scene, to avoid using too much memory. `stackstac` handles all of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import planetary_computer\n",
    "\n",
    "signed_item = planetary_computer.sign(sentinel_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778aab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stackstac\n",
    "\n",
    "assets = [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B09\"]\n",
    "data = (\n",
    "    stackstac.stack(\n",
    "        signed_item.to_dict(),\n",
    "        assets=assets,\n",
    "        epsg=labels.rio.crs.to_epsg(),  # reproject to the labels CRS\n",
    "        bounds=labels.rio.bounds(),  # crop to the labels' bounds\n",
    "        resolution=10,  # resample all assets to the highest resolution\n",
    "        dtype=\"float32\",\n",
    "    )\n",
    "    .where(lambda x: x > 0)\n",
    "    .squeeze()\n",
    ")  # 0 is nodata\n",
    "\n",
    "assert data.shape[1:] == labels.shape\n",
    "\n",
    "data = data.compute()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0263d",
   "metadata": {},
   "source": [
    "Our shapes match (256 x 256 patch), but the labels don't quite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b8278",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb19aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc5980",
   "metadata": {},
   "source": [
    "They're off by half a pixel (5 units in coordinate space). This probably just comes from one dataset labeling the center of the pixel and the other labeling the top-left corner. It'd be good to verify that. In the meantime, let's just update the labels on `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87199ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.assign_coords(\n",
    "    y=lambda ds: (ds.y - 5).round(),  # fix half-pixel label issue\n",
    "    x=lambda ds: (ds.x + 5).round(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2df95",
   "metadata": {},
   "source": [
    "Now we should be all set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9107c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data2.x.data == labels.x.data).all()\n",
    "assert (data2.y.data == labels.y.data).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32781963",
   "metadata": {},
   "source": [
    "And we can spot-check out transformation visually. Let's plot the Red band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.sel(band=\"B04\").plot.imshow(figsize=(8, 8), cmap=\"Reds\", add_colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ec618",
   "metadata": {},
   "source": [
    "Which looks pretty close to what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b35ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.plot.imshow(figsize=(8, 8), add_colorbar=False, cmap=\"tab10\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6430f89a",
   "metadata": {},
   "source": [
    "## Train a Baseline Model\n",
    "\n",
    "We've come this far, we might as well do some machine learning! We'll use a scikit-learn KNeighborsClassfier ([User Guide](https://scikit-learn.org/stable/modules/neighbors.html#classification), [API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) to establish a baseline model for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de06f1f",
   "metadata": {},
   "source": [
    "As usual with scikit-learn, the estimator expects an intput array of `(n_samples, n_features)`. In this case a \"sample\" will be a single pixel, and the feature will be all the bands. We'll get to that shape by stacking all the the pixels into one long array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039dd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data2.stack(pixel=(\"y\", \"x\")).T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748d33a",
   "metadata": {},
   "source": [
    "We'll also reshape the labels to be a 1-D array `(n_features,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels.stack(pixel=(\"y\", \"x\"))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.indexes[\"pixel\"].equals(y.indexes[\"pixel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b930d",
   "metadata": {},
   "source": [
    "### Exercise: Train the model\n",
    "\n",
    "In this exercise, we'll train the KNeighborsClassfier model on a subset of the data. We'll then score it on the remainder \n",
    "of the data.\n",
    "\n",
    "First, split `X` and `y` into train and test parts. See [Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split X and y\n",
    "%load solutions/train_test_split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267216a1",
   "metadata": {},
   "source": [
    "Now fit the model on `X_train` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the KNeighborsClassifier\n",
    "clf = sklearn.neighbors.KNeighborsClassifier()\n",
    "clf.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45202c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/fit_knn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db3bc5",
   "metadata": {},
   "source": [
    "Score that model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the score on the training dataset\n",
    "clf.score(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cac3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/score_knn_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2208a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the score on the test dataset\n",
    "clf.score(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ae535",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/score_knn_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2ce00",
   "metadata": {},
   "source": [
    "We're overfitting a bit (89% on our training dataset, 84% on our test), but not too bad. Let's visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738efd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio.plot\n",
    "\n",
    "yhat = clf.predict(X)\n",
    "\n",
    "rasterio.plot.show(yhat.reshape(labels.shape), cmap=\"tab10\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6dc9a",
   "metadata": {},
   "source": [
    "Not *awful*, but certainly room for improvement. A few things to note:\n",
    "\n",
    "1. We haven't done any fancy preprocessing (e.g. mosaicing scenes to remove clouds) that could improve this model.\n",
    "2. We haven't done any fancy hyper-parameter tuning that could improve this model.\n",
    "3. We aren't using the spatial \"context\" of each pixel at all, which leads to the splotchiness in our predictions. If a pixel is surrounded on all sides by Wheat pixels, then it's probably a Wheat pixel. But our current model has no idea about the neighboring pixels.\n",
    "4. There's a lot of variation across the scenes in the label data. Our model hasn't seen any of those, so likely won't generalize well to them. We'd need to train on more data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1792dda3292141c6b54a76ec61e737eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "layout": "IPY_MODEL_f741701f6a0b4b46836126f9bf10ca7e"
      }
     },
     "f741701f6a0b4b46836126f9bf10ca7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
