jupyterhub:
  proxy:
    https:
      enabled: true
      hosts:
        - "esip2021.westeurope.cloudapp.azure.com"
      letsencrypt:
        contactEmail: "taugspurger@microsoft.com"
    service:
      annotations:
        # Update this with your hub's name.
        service.beta.kubernetes.io/azure-dns-label-name: "esip2021"

  hub:
    # Disable hub network Policy, so that the dask gateway server API can reach the hub directly
    # Not required for dask-gateway>0.9.0
    # https://github.com/dask/helm-chart/issues/142
    networkPolicy:
      enabled: false

    # dask-gateway service added in secrets.yaml

    config:
      JupyterHub:
        authenticator_class: dummy
        # password is set in secrets.yaml

  singleuser:
    image:
      name: "mcr.microsoft.com/planetary-computer/gpu-pytorch"
      tag: "2021.07.13.0"
    startTimeout: 1200  # 20 * 60s = 10 minutes
    cpu:
      guarantee: 3
      limit: 4
    memory:
      guarantee: "12G"
      limit: "24G"

    extraResource:
      # guarantees:
      #   nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1

    defaultUrl: "/lab/tree/workshop/index.ipynb"
    lifecycleHooks:
      postStart:
        exec:
          command:
            [
              "/srv/conda/envs/notebook/bin/gitpuller",
              "https://github.com/TomAugspurger/esip-summer-2021-geospatial-ml",
              "main",
              "workshop",
            ]

    extraEnv:
      DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE: '{JUPYTER_IMAGE_SPEC}'
      DASK_DISTRIBUTED__DASHBOARD__LINK: '/user/{JUPYTERHUB_USER}/proxy/{port}/status'
      DASK_LABEXTENSION__FACTORY__MODULE: 'dask_gateway'
      DASK_LABEXTENSION__FACTORY__CLASS: 'GatewayCluster'
      NVIDIA_DRIVER_CAPABILITIES: 'compute,utility'
      # GDAL / Rasterio environment variables for performance
      GDAL_DISABLE_READDIR_ON_OPEN: "EMPTY_DIR"
      GDAL_HTTP_MERGE_CONSECUTIVE_RANGES: "YES"
      GDAL_HTTP_MAX_RETRY: "5"

dask-gateway:
  gateway:
    # auth set in secrets.yaml
    backend:
      worker:
        # Ensure workers are scheduled on the worker pool
        extraPodConfig:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: "k8s.dask.org/dedicated"
                    operator: "In"
                    values:
                      - "worker"

          tolerations:
            # allow workers to be scheduled on the worker pool, which has preemptible nodes.
            - key: "k8s.dask.org/dedicated"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"
            - key: "k8s.dask.org_dedicated"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"
            - key: "kubernetes.azure.com/scalesetpriority"
              operator: "Equal"
              value: "spot"
              effect: "NoSchedule"

    extraConfig:
      01-optionHandler: |
          # Configure options to
          # 1. Have the default worker image match the singleuser image
          # 2. Place bounds on worker CPU and Memory requests
          # 3. Accept a mapping of environment variables to pass to workers.
          from dask_gateway_server.options import Options, Float, String, Mapping
          def cluster_options(user):
              def option_handler(options):
                  if ":" not in options.image:
                      raise ValueError("When specifying an image you must also provide a tag")

                  return {
                      "worker_cores": 0.88 * min(options.worker_cores / 2, 1),
                      "worker_cores_limit": options.worker_cores,
                      "worker_memory": "%fG" % (0.95 * options.worker_memory),
                      "worker_memory_limit": "%fG" % options.worker_memory,
                      "image": options.image,
                      "environment": options.environment,
                  }
              return Options(
                  Float("worker_cores", 1, min=1, max=16, label="Worker Cores"),
                  Float("worker_memory", 2, min=1, max=32, label="Worker Memory (GiB)"),
                  String("image", default="pangeo/pangeo-notebook:latest", label="Image"),
                  Mapping("environment", {}, label="Environment Variables"),
                  handler=option_handler,
              )
          c.Backend.cluster_options = cluster_options